Optimization based tracking methods have been widely successful by integrating a target model prediction module, providing effective global reasoning by minimizing an objective function. While this inductive bias integrates valuable domain knowledge, it limits the expressivity of the tracking network. In this work, we therefore propose a tracker architecture employing a Transformer-based model prediction module. Transformers capture global relations with little inductive bias, allowing it to learn the prediction of more powerful target models. We further extend the model predictor to estimate a second set of weights that are applied for accurate bounding box regression. The resulting tracker relies on training and on test frame information in order to predict all weights transductively. We train the proposed tracker end-to-end and validate its performance by conducting comprehensive experiments on multiple tracking datasets. Our tracker sets a new state of the art on three benchmarks, achieving an AUC of 68.5% on the challenging LaSOT [20] dataset. The code and trained models are available at https://github.com/visionml/pytracking
---------------------
Generic visual object tracking is one of the fundamental problems in computer vision. The task involves estimating the state of the target object in every frame of a video sequence, given only the initial target location. One of the key problems in object tracking is learning to robustly detect the target object, given the scarce annotation. Among exiting methods, Discriminative Correlation Filters (DCF) [1, 5, 13, 14, 24, 29, 46, 54] have achieved much success. These approaches learn a target model to localize the target in each frame, by minimizing a discriminative objective function. The target model, often set to a convolutional kernel, provides a compact and generalizable representation of the tracked object, leading to the popularity of DCFs.
---------------------
The objective function in DCF integrates both foreground and background knowledge over the previous
---------------------
Figure 1. Performance improvements when transforming the model optimizer based tracker SuperDiMP [12] ( ) step-by-step. First, we replace the model optimizer by a Transformer based model predictor ( ). Secondly, we replace the probabilistic IoUNet by a new regressor and predict its weights with the same model predictor ( ). The performance (success AUC) is reported on NFS [23] and LaSOT [20] and compared with recent trackers ( ). ToMP-50 and ToMP-101 refer to the different employed backbones ResNet-50 [28] and ResNet-101 [28]. frames, providing effective global reasoning when learning the model. However, it also imposes severe inductive bias on the predicted target model. Since the target model is obtained by solely minimizing an objective over the previous frames, the model predictor has limited ﬂexibility. For instance, it cannot integrate any learned priors in the predicted target model. On the other hand, Transformers have also been shown to provide strong global reasoning across multiple frames, thanks to the use of self and cross attention. Consequently, Transformers have been applied to generic object tracking [7, 59, 63, 67] with considerable success.
---------------------
In this work, we propose a novel tracking framework that aims at bridging the gap between DCF and Transformer based trackers. Our approach employs a compact target model for localizing the target, as in DCF. The weights of this model are however obtained using a Transformer-based model predictor, allowing us to learn more powerful target models, compared to DCFs. This is achieved by introducing novel encodings of the target state, allowing the Transformer to effectively utilize this information. We further extend our model predictor to generate weights for a bounding box regressor network, in order to condition its predictions on the current target. Our proposed approach ToMP obtains signiﬁcant improvement in tracking performance compared to state-of-the-art DCF-based methods, while also outperforming recent Transformer based trackers (see Fig. 1). Contributions: In summary, our main contributions are the following: i) We propose a novel Transformer-based model prediction module in order to replace traditional optimization based model predictors. ii) We extend the model predictor to estimate a second set of weights that are applied for bounding box regression. iii) We develop two novel encodings that incorporate target location and target extent allowing the Transformer-based model predictor to utilize this information. iv) We propose a parallel two stage tracking procedure at test time to decouple target localization and bounding box regression in order to achieve robust and accurate target detection. v) We perform a comprehensive set of ablation experiments to assess the contribution of each building block of our tracking pipeline and evaluate it on seven tracking benchmarks. The proposed tracker ToMP sets a new state of the art on three including LaSOT [20] where it achieves an AUC of 68.5% (see Fig. 1). In addition we show that our tracker ToMP outperforms other Transformer based trackers for every attribute of LaSOT [20].
---------------------
Discriminative Model Prediction: DCF based approaches learn a target model to distinguish the target from background by minimizing an objective. For long Fouriertransform based solvers were predominant for DCF based trackers [5, 15, 29, 46]. Danelljan et al. [13] employed a two layer Perceptron as target model and use Conjugate Gradient to solve the optimization problem. Recently, multiple methods have been introduced that enable end-toend training by casting the tracking problem into a metalearning problem [1, 58, 72]. These methods are based on the idea of unrolling the iterative optimization algorithm for a ﬁxed number of iterations and to integrate it in the tracking pipeline to allow end-to-end training. Bhat et al. [1] learn a discriminative feature space and predict the weights of the target model based on the target state in the initial frame and reﬁne the weights with an optimization algorithm. Transformers for Tracking: Recently, several trackers have been introduced that use Transformers [7, 59, 63, 67]. Transformers are typically employed to predict discriminative features to localize the target object and regress its bounding box. The training features are processed by the Transformer Encoder whereas the Transformer Decoder fuses training and test features using cross attention layers to compute discriminative features [7, 59, 67].
---------------------
DTT [67] feeds these features to two networks that predict the location and the bounding box of the target. In contrast, TransT [7] employs a feature fusion network that consists of multiple self and cross attention modules. The fused output features are fed into a target classiﬁer and a bounding box regressor. TrDiMP [59] adopts the DiMP [1] model predictor to produce the model weights given the output features of the Transformer Encoder as training samples. Afterwards, the target model computes the target score map by applying the predicted weights on the output features produced by the Transformer Decoder. TrDiMP adopts the probabilistic IoUNet [16] for bounding box regression. Similar to our tracker, TrDiMP encodes target state information but integrates it via two different cross attention modules in the Decoder instead of using two encoding modules in front of the Transformer.
---------------------
In contrast to the aforementioned Transformer based trackers, STARK [63] adopts the Transformer architecture from DETR [6]. Instead of fusing the training and test features in the Transformer Decoder they are stacked and processed jointly by the full Transformer. A single objectquery then produces the Decoder output that is fused with the Transformer Encoder features. These features are then further processed to directly predict the bounding box of the target. In contrast, our tracker employs the same Transformer architecture from DETR [6] but to replace the model optimizer. In the end, our resulting Transformer-based model predictor estimates the weights of two separate models: the target classiﬁer and the bounding box regressor.
---------------------
In this work, we propose a Transformer-based target model prediction network for tracking called ToMP. We ﬁrst revisit existing optimization based model predictors and discuss their limitations in Sec. 3.1. Next, we describe our Transformer-based model prediction approach in Sec. 3.2. We extend this approach to perform joint target classiﬁcation and bounding box regression in Sec. 3.3. Finally, we detail our ofﬂine training procedure and online tracking pipeline in Sec. 3.4 and Sec. 3.5, respectively.
---------------------
One of the popular paradigms for visual object tracking is discriminative model prediction based tracking. These approaches, visualized in Fig. 2a, use a target model to localize the target object in the test frame. The weights (parameters) of this target model are obtained from the model optimizer, using the training frames and their annotation. While a variety of target models are used in the literature [1, 13, 33, 46, 54, 58, 72], discriminative trackers share a common base formulation to produce the target model weights. This involves solving an optimization problem such that the target model produces the desired target states y∈ Y for the training samples S∈ {(x, y)}. Here, x∈ X refers to a deep feature map of frame i and m
---------------------
Figure 2. Comparison between trackers that employ optimization based model prediction and our Transformer-based model prediction. The model optimizer [] in Fig. 2a is replaced by the model predictor in Fig. 2b that consists of the proposed modules [,,,]. denotes the total number of training frames. The optimization problem reads as follows, w = arg min
---------------------
Here, the objective consists of the residual function f which computes an error between the target model output h( ˜w; x) and the ground truth label y. g( ˜w) denotes the regularization term weighted by a scalar λ, while w represents the optimal weights of the target model. Note that the training set Scontains the annotated ﬁrst frame, as well as the previous tracked frames with the tracker’s predictions being used as pseudo-labels.
---------------------
Learning the target model by explicitly minimizing the objective of (1) provides a robust target model that can distinguish the target from the previously seen background. However, such a strategy suffers from notable limitations. The optimization based methods compute the target model using only limited information available in previously tracked frames. That is, they cannot integrate learned priors in the target model prediction so as to minimize future failures. Similarly, these methods typically lack the possibility to utilize the current test frame in a transductive manner when computing the model weights to improve tracking performance. The optimization based methods also require setting multiple optimizer hyper-parameters, and can overﬁt/underﬁt on the training samples. Another limitation of optimization based trackers is their procedure that produces the discriminative features. Usually, the features provided to the target model are simply the extracted test features. Instead of reinforced features by using the target state information contained in the training frames. Extracting such enhanced features would allow reliable differentiation between the target and background regions in the test frame.
---------------------
In order to overcome the aforementioned limitations of optimization based target localization approaches, we propose to replace the model optimizer by a novel target model predictor based on Transformers (see Fig. 2b). Instead of explicitly minimizing an objective as stated in (1), our approach learns to directly predict the target model purely from data by end-to-end training. This allows the model predictor to integrate target speciﬁc priors in the predicted model so that it can focus on characteristic features of the target, in addition to the features that allow to differentiate the target from the seen background. Furthermore, our model predictor also utilizes the current test frame features, in addition to the previous training features, to predict the target model in a transductive manner. As a result, the model predictor can utilize the current frame information to predict a more suitable target model. Finally, instead of applying the target model on a ﬁxed feature space, deﬁned by the pre-trained feature extractor, our approach can utilize the target information to dynamically construct a more discriminative feature space for every frame.
---------------------
An overview of the proposed tracker employing the Transformer-based model prediction is shown in Fig. 2b. Similar to the optimization based trackers, it consists of a test and training branch. We ﬁrst encode the target state information in the training frames and fuse it with the deep image features []. Similarly, we also add an encoding to the test frame in order to mark it as test frame []. The features from both the training and test branches are then jointly processed in the Transformer Encoder [] that produces enhanced features by reasoning globally across frames. Next, the Transformer Decoder [] predicts the target model weights [ ] using the output of the Transformer Encoder. Finally, the predicted target model is applied on the enhanced test frame features to localize the target. Next, we describe the main components in our tracking pipeline.
---------------------
Target Location Encoding: We propose a target location encoding that allows the model predictor to incorporate the target state information from the training frames, when predicting the target model. In particular, we use the embedding e∈ Rthat represents foreground. Together with a Gaussian y∈ Rcentered at the target location,
---------------------
Figure 3. Overview of the entire ToMP tracking pipeline for joint model prediction. First, the training [] and test [] features are extracted using a backbone. Then the target location [] and bounding box [] encodings are added to the training features. For the test frame the test embedding is encoded [] and added to the test features. The features are then concatenated and jointly processed by the Transformer-based model predictor that produces the weights used for target classiﬁcation [] and bounding box regression []. we deﬁne the target encoding function where ”·” denotes point-wise multiplication with broadcasting. Note, that H= s·H and W= s·W correspond to the spatial dimension of the image patch and s to the stride of the backbone network used to extract the deep features x ∈ R. Next, we combine the target encoding and the deep image features x as follows
---------------------
This provides us the training frame features v∈ R which contain encoded target state information. Similarly, we also add a test encoding to identify the features corresponding to the test frame as, where µ(·) repeats the token efor each patch of x. model using the foreground and background information from both the training, as well as the test frames. To achieve this, we use a Transformer Encoder [6, 56] module to ﬁrst jointly process the features from the training frames and the test frame. The Transformer Encoder serves two purposes in our approach. First, as described later, it computes the features used by the Transformer Decoder module to predict the target model. Secondly, inspired by STARK [63], our Transformer Encoder also outputs enhanced test frame features, which serve as the input to the target model when localizing the target.
---------------------
Given multiple encoded training features v∈ Rand an encoded test feature v∈ R, we reshape the features to Rand concatenate all m training features vand the test feature valong the ﬁrst dimension. These concatenated features are then processed jointly in a Transformer Encoder
---------------------
The Transformer Encoder consists of multi-headed selfattention modules [56] that enable it to reason globally across a full frame and even across multiple training and test frames. In addition, the encoded target state identiﬁes foreground and background regions and enables the Transformer to differentiate between both regions. Transformer Decoder: The outputs of the Transformer Encoder (zand z) are used as inputs for the Transformer Decoder [6, 56] to predict the target model weights
---------------------
Note that the inputs zand zare obtained by jointly reasoning over the whole training and test samples, allowing us to predict a discriminative target model. We use the same learned foreground embedding eas used for target state encoding as input query of the Transformer Decoder such that the Decoder predicts the target model weights. Target Model: We use the DCF target model to obtain the target classiﬁcation scores
---------------------
Here, the weights of the convolution ﬁlter w ∈ Rare predicted by the Transformer Decoder. Note that the target model is applied on the output test features zof the Transformer Encoder. These features are obtained after joint processing of training and test frames, and thus support the target model to reliably localize the target.
---------------------
In the previous section, we presented our Transformer based architecture for predicting the target model. Although the target model can localize the object center in each frame, a tracker needs to also estimate an accurate bounding box of the target. DCF based trackers typically employ a dedicated bounding box regression network [13] for this task. While it is possible to follow a similar strategy, we decide to predict both models jointly since target localization and bounding box regression are related tasks that can beneﬁt from one another. In order to achieve this, we extend our model as follows. First, instead of only using the target center location when generating the target state encoding, we also encode target size information to provide a richer input to our model predictor. Secondly, we extend our model predictor to estimate weights for a bounding box regression network, in addition to the target model weights. The resulting tracking architecture is visualized in Fig. 3. Next, we describe each of these changes in detail. Target Extent Encoding: In addition to the extracted deep image features xand the target location encoding ψ(y, e), we add another encoding to incorporate information about the bounding box of the target. In order to encode the bounding box b= {b, b, b, b} encompassing the target object in the training frame i, we adopt the ltrb representation [22, 55, 62, 67]. First, we map each location (j, j) on the feature map xback to the image domain using (k, k) = (bc+ s·j, bc+ s·j). Then, we compute the normalized distance of each remapped location to the four sides of the bounding box bas follows, where W= s ·W and H= s ·H. These four sides are used to produce the dense bounding box representation d = (l, t, r, b), where d ∈ R. In this representation, we encode the bounding box using a Multi-Layer Perceptron (MLP) φ and thereby increase the number of dimensions from 4 to C before adding the obtained encoding to Eq. (3) such that
---------------------
Here, vis the resulting feature map which is used as input to the Transformer Encoder, see Fig. 3. Model Prediction: We extend our architecture to predict weights for the target model, as well as bounding box regression. Concretely, we pass the output w of the Transformer Decoder through a linear layer to obtain the weights for bounding box regression wand target classiﬁcation w. The weights ware then directly used within the target model h(w; z) as before. The weights w, on the other hand, are used to condition the output test features zof the Transformer Encoder with target information for bounding box regression, as explained next. Bounding Box Regression: To make the encoder output features ztarget aware, we follow Yan et al. [63] and ﬁrst compute an attention map w∗ zusing the predicted weights w. The attention weights are then multiplied point-wise with the test features zbefore feeding them into a Convolutional Neural Network (CNN). The last layer of the CNN uses an exponential activation function to produce the normalized bounding box prediction in the same ltrb representation as described in Eq. (8). In order to obtain the ﬁnal bounding box estimation, we ﬁrst extract the center location by applying the argmax(·) function on the target score map ˆypredicted by the target model. Next, we query the dense bounding box predictionˆdat the center location of the target object to obtain the bounding box. We use two dedicated networks for target localization and bounding box regression in contrast to Yan et al. [63] that uses one network trying to predict both. This allows us as explained in Sec. 3.5 to decouple target localization from bounding box regression during tracking.
---------------------
In this section, we describe the protocol to train the proposed tracker ToMP. Similar to recent end-to-end trained discriminative trackers [1, 16], we sample multiple training and test frames from a video sequence to form training subsequences. In particular, we use two training frames and one test frame. In contrast to recent Transformer based trackers [7, 63, 67] but similar to DCF based trackers [1, 13, 16], we keep the same spatial resolution for training and test frames. We pair each image Iwith the corresponding bounding box b. We use the target state of the training frames to encode target information and use the bounding box of the test frame only to supervise training by computing two losses based on the predicted bounding boxes and the derived center location of the target in the test frame.
---------------------
We employ the target classiﬁcation loss from DiMP [1] that consists of different losses for background and foreground regions. Further, we employ the generalized Intersection over Union loss [52] using the ltrb bounding box representation [55] to supervise bounding box regression where λand λare scalars weighting the contribution of each loss. Note that in contrast to FCOS [55] and related trackers [22] we omit an additional centerness loss since it would be redundant in addition to our classiﬁcation loss that serves the same purpose. A detailed study examining the impact of centerness is available in the supplementary. Training Details: We train our tracker on the training splits of the LaSOT [20], GOT10k [31], Trackingnet [50] and MS-COCO [43] datasets. We sample 40k sub-sequences and train for 300 epochs on two Nvidia Titan RTX GPUs. We use ADAMW [45] with a learning rate of 0.0001 that we decay by a factor of 0.2 after 150 and 250 epochs and weight decay of 0.0001. We set λ= 100 and λ= 1. We construct a training sub-sequence by randomly sampling two training frames and a test frame from a 200 frame window within a video sequence. We then extract the image patches after randomly translating and scaling the image relative to the target bounding box. Moreover, we use random image ﬂipping and color jittering for data augmentation. We set the spatial resolution of the target scores to 18 × 18 and set the search area scale factor to 5.0. Further training and architecture details are provided in the supplementary Sec. A.
---------------------
During tracking, we use the annotated ﬁrst frame, as well as previously tracked frames as our training set S. While we always keep the initial frame and its annotation, we include one previously tracked frame and replace it with the most recent frame that achieves a target classiﬁer conﬁdence higher than a threshold. Hence, the training set S contains at most two frames.
---------------------
We observed that incorporating previous tracking results in Simproves the target localization considerably.. However, including predicted bounding box estimations degrades the bounding box regression performance due to inaccurate predictions, see Sec. 4.1. Hence, we run the model predictor twice. First, we include intermediate predictions in Sto obtain the classiﬁer weights. In the second pass, we only use the annotated initial frame to predict the bounding box. Note that for efﬁciency both steps can be performed in parallel in a single forward pass. In particular, we reshape the feature map corresponding to two training and one test frame to a sequence and duplicate it. Then, we stack both in the batch dimension to process them jointly with the model predictor. To only allow attention between the initial frame with ground truth annotation and the test frame when predicting the model for bounding box regression, we make use of the so-called keypadding mask that allows to ignore certain keys when computing attention.
---------------------
We evaluate our proposed tracking architecture ToMP on seven benchmarks. Our approach is based on PyTorch 1.7 and is developed within the PyTracking [12] frame work. PyTracking is available under the GNU GPL 3.0 license. On a single Nvidia RTX 2080Ti GPU, ToMP-101 and ToMP-50 achieve 19.6 and 24.8 FPS and use a ResNet101 [28] and ResNet-50 [28] as backbone respectively.
---------------------
We perform a comprehensive analysis of the proposed tracker. First, we analyze the contribution of the different proposed target state encodings and then examine the effect of different inference settings. Finally, we report the performance achieved when replacing the target classiﬁer or the bounding box regressor of SuperDiMP with ours. All ablation experiments in this part use a ResNet-50 as backbone. Target State Encoding: In order to analyze the effect of the different target state encodings we train different variants of our network and evaluate them on multiple datasets. The ﬁrst ﬁve rows of Tab. 1 correspond to versions with different target location encodings. All other settings are kept the same. In addition to the foreground and test embedding, we include a learned background embedding (instead of setting e= 0) to our analysis as follows: ψ(y, e, e) = y· e+ (1 − y) · e. However, Tab. 1 shows (4vs. 5 row) that adding such a learned background embedding decreases the tracking performance. We further observe that setting the foreground embedding e= 0 (1row) and only relying on the target extent encoding φ(·) still achieves high tracking performance but clearly lacks behind all other versions that include the foreground embedding. We conclude that using only the foreground encoding eand the test encoding eleads to the best performance (4row).
---------------------
In the second part of Tab. 1 we choose the best settings for the target location encoding and remove either the target extent encoding φ(·) or decouple the Transformer Decoder query from the foreground embedding e. We observe that using a separate query (6row) decreases the overall performance. Similarly, we notice that incorporating target extent information via the proposed encoding is crucial. Otherwise, the performance drops signiﬁcantly (7row). Model Predictor: Since our model predictor estimates two different model weights, it seems natural to use two different Transformer queries: one to produce the target model
---------------------
Table 1. For e, eand elearning the embedding is denoted by X whereas 7 means setting it to zero. Using the encoding φ(·) is denoted by X whereas 7 refers to omitting it. For q= ethe symbol X means sharing the learned embedding efor encoding and querying the Decoder wheres 7 means learning two separate embeddings for both tasks. (Our ﬁnal model is in the 4row).
---------------------
Table 2. Analysis of different model predictor architectures and its impact on the tracking performance in terms of success AUC.
---------------------
Table 3. Analysis of different inference settings an of their impact on the tracking performance in terms of success AUC.
---------------------
Table 4. Impact of replacing DiMP [1] and the probabilistic IoUNet [16] with ToMP for localization and box regression. weights and the other to obtain the bounding box regressor weights. However, this involves decoupling the query from the foreground embedding eand the experiments in Tab. 2 show a signiﬁcant performance drop for this case. Inference Settings: During online tracking, we use the initial frame and its annotation as training frames. In addition, we include the most recent frame and its target prediction if the classiﬁer conﬁdence is above a certain threshold. Tab. 3 shows that including previous tracking results leads to higher tracking performance than using only the initial frame. Disabling the described two stage model prediction approach and predicting the weights of the target model and bounding box regressor at once decreases the tracking performance drastically (-5.6 AUC on LaSOT). The reason is the sensitivity of the bounding box predictor to inaccurate predicted boxes that are encoded and used for training. model predictor can estimate model weights for the target model and bounding box regressor. In this part, we will transform an optimization based tracker step-by-step to assess the impact of each transformation step. Tab. 4 shows that replacing the model optimizer in SuperDiMP (1row) with our proposed model predictor to only predict the target model (2row) outperforms SuperDiMP on three out of four datasets. Our tracker ToMP that jointly predicts model weights for target localization and bounding box regression (3row) achieves the best performance on all four datasets. We conclude that predicting the weights of the target model improves the performance and likewise predicting the weights of the bounding box regressor. Note that we report the average over ﬁve runs for all trackers based on the probabilistic IoUNet due to its stochasticity.
---------------------
We compare our tracker ToMP on seven tracking benchmarks. The same settings and parameters are used for all datasets. We recompute the metrics of all trackers using the raw predictions if available or otherwise report the results given in the respective paper. LaSOT [20]: First, we compare ToMP on the large-scale LaSOT dataset (280 test sequences with 2500 frames on average). The success plot in Fig. 5a shows the overlap precision OPas a function of the threshold T . Trackers are ranked w.r.t. their area-under-the-curve (AUC) score, shown in the legend. Tab. 5 shows more results including precision and normalized precision for each tracker. Both versions of ToMP with different backbones outperform the recent trackers STARK [63], TransT [7], TrDiMP [59] and DTT [67] in AUC and sets a new state-of-the-art result. Note that even ToMP with ResNet-50 outperforms STARKST101 with ResNet-101 (67.6 vs 67.1). Fig. 4 shows the success AUC gain of ToMP compared to recent Transformer based trackers for different attributes annotated in LaSOT [20]. We want to highlight that ToMP outperforms TransT [7] and TrDiMP [59] on each attribute by more than one percent point. Similarly, ToMP achieves higher performance than STARK-ST101 for every attribute. It achieves the highest gain over STARK for Background Clutter, showing the disadvantage of using small templates instead of training frames with a large ﬁeld of view that allow not only to leverage target, but also background information.
---------------------
LaSOT. It only contains test sequences assigned to 15 new classes with 10 videos each. The sequences contain 2500 frames on average showing challenging tracking scenarios of small, fast moving objects with distrac-
---------------------
Table 5. Comparison on the LaSOT [20] test set ordered by AUC.
---------------------
Figure 4. Per attribute analysis on LaSOT [20] between ToMP and recent Transformer based trackers. The bar heights correspond to the gain of our tracker and the legend shows the average gain.
---------------------
Figure 5. Success plots, showing OP, on LaSOT [20] and LaSOTExtSub [19] and AUC is reported in the legend.
---------------------
Table 6. Comparison on the TrackingNet [50] test set.
---------------------
Table 7. Comparison with the state of the art on the OTB-100 [61], NFS [23] and UAV123 [49] datasets in terms of AUC score. tors present. Fig. 5b shows the success plot where the results of most trackers are obtained from [19], e.g., DaSiamRPN [74], SiamRPN++ [39], ATOM [13], DiMP [1] and LTMU [11]. ToMP exceeds the performance of all trackers except KeepTrack [48] that employs explicit distractor matching between frames. In particular, we outperform SuperDiMP [12] that uses a model optimizer (+2.2%). TrackingNet [50]: We evaluate ToMP on the large-scale TrackingNet dataset that contains 511 test sequences without publicly available ground-truth. An online evaluation server is used to obtain the tracking metrics shown in Tab. 6 by submitting the raw tracking results. Both versions of ToMP achieve competitive results close to the current state of the art. In particular, ToMP-101 achieves the second best performance in terms of AUC behind STARK [63], outperforming other Transformer based trackers such as TransT [7] and TrDiMP [59]. UAV123 [49]: The UAV dataset consists of 123 test videos that contains small objects, target occlusion, and distractors. Tab. 7 shows the achieved results in terms of success AUC. Again, ToMP achieves competitive results compared to the current state of the art achieved by KeepTrack [48]. OTB-100 [61]: We also report results on the OTB-100 dataset that contains 100 short sequences. Multiple trackers achieve results above 70% AUC. Among them are both versions of ToMP, see Tab. 7. ToMP achieve the same performance as SuperDiMP [12] but slightly higher results than TransT [7] and slightly lower than TrDiMP [59]. NFS [23]: We compete on the NFS dataset (30FPS version) containing 100 test videos. It contains fast motions
---------------------
Table 8. Comparison to the state of the art of bounding box only methods on VOT2020ST [35] in terms of EAO score. and challenging sequences with distractors. Both versions of ToMP exceed the performance of the current best method KeepTrack [48] by +0.5% and +0.3%, see Tab. 7. VOT2020 [35]: Finally, we evaluate on the 2020 edition of the Visual Object Tracking short-term challenge. We compare with the top methods in the challenge [35], as well as more recent methods. The dataset contains 60 videos annotated with segmentation masks. Since ToMP produces bounding boxes we only compare with trackers that produce the bounding boxes as well. The trackers are evaluated following the multi-start protocol and are ranked according to the EAO metric that is based on tracking accuracy and robustness, deﬁned using IoU overlap and failure rate respectively. The results in Tab. 8 show that ToMP-101 achieves the best overall performance, with the highest robustness and competitive accuracy compared to previous methods.
---------------------
Transformer Encoders consist of self-attention layers that compute similarity matrices between multiple training and test frame features and thus lead to a large memory footprint that impacts training and inference run-time. Thus, in future work this limitation should be addressed by evaluating alternatives such as [32, 34, 53] aiming at decreasing the memory burden. Another limiting factor of ToMP arises from challenging tracking sequences. In particular, distractors present while the target is occluded is a typical failure scenario of ToMP, since it is lacking explicit distractor handling as in KeepTrack [48].
---------------------
We propose a novel tracking architecture employing a Transformer-based model predictor. The model predictor estimates the weights of the compact DCF target model to localize the target in the test frame. In addition, the predictor produces a second set of weights used for precise bounding box regression. To achieve this, we develop two new modules that encode target location and its bounding box in the training features. We conduct comprehensive experimental validation and analysis of ToMP on several challenging datasets, and set a new state of the art on three. Acknowledgments: This work was partly supported by the ETH Z¨urich Fund (OK), Siemens Smart Infrastructure, the ETH Future Computing Laboratory (EFCL) ﬁnanced by a gift from Huawei Technologies, an Amazon AWS grant, and an Nvidia hardware grant.
---------------------
