[[0, 'Abstract Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems.', 116],
 [1, 'To this date, most of the successful SRL systems were built on top of some form of parsing results (Koomen et al.', 32], 
 [2, ', 2005;', 278], 
 [3, 'Palmer et al.', 279], 
 [4, ', 2010;', 280], 
 [5, 'Pradhan et al.', 281], 
 [6, ', 2013), where pre-deﬁned feature templates over the syntactic structure are used.', 282], 
 [7, 'The attempts of building an end-to-end SRL learning system without using parsing were less successful (Collobert et al.', 162], 
 [8, ', 2011).', 283], [9, 'In this work, we propose to use deep bi-directional recurrent network as an end-to-end system for SRL.', 237], [10, 'We take only original text information as input feature, without using any syntactic knowledge.', 284], [11, 'The proposed algorithm for semantic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F 1 score of 81.', 105], [12, '07.', 285], [13, 'This result outperforms the previous state-of-the-art system from the combination of different parsing trees or models.', 68], [14, 'We also obtained the same conclusion with F 1 = 81.', 286], [15, '27 on CoNLL2012 shared task.', 205], [16, 'As a result of simplicity, our model is also computationally efﬁcient that the parsing speed is 6.', 39], [17, '7k tokens per second.', 287], [18, 'Our analysis shows that our model is better at handling longer sentences than traditional models.', 244], [19, 'And the latent variables of our model implicitly capture the syntactic structure of a sentence.', 20], [20, 'Semantic role labeling (SRL) is a form of shallow semantic parsing whose goal is to discover the predicate-argument structure of each predicate in a given input sentence.', 2], [21, 'Given a sentence, for each target verb (predicate) all the constituents in the sentence which ﬁll a semantic role of the verb have to be recognized.', 17], [22, 'Typical semantic arguments include Agent, Patient, Instrument, etc.', 288], [23, ', and also adjuncts such as Locative, Temporal, Manner, Cause, etc.', 289], [24, '. SRL is useful as an intermediate step in a wide range of natural language processing (NLP) tasks, such as information extraction (Bastianelli et al.', 0], [25, ', 2013), automatic document categorization (Persson et al.', 290], [26, ', 2009) and questionanswering (Dan and Lapata, 2007;', 291], [27, 'Surdeanu et al.', 292], [28, ', 2003;', 293], [29, 'Moschitti et al.', 294], [30, ', 2003).', 295], [31, 'SRL is considered as a supervised machine learning problem.', 109], [32, 'In traditional methods, linear classiﬁer such as SVM is often employed to perform this task based on features extracted from the training corpus.', 84], [33, 'Actually, people often treat this problem as a multi-step classiﬁcation task.', 58], [34, 'First, whether an argument is related to the predicate is determined;', 296], [35, 'next the detail relation type was decided(Palmer et al.', 297], [36, ', 2010).', 298], [37, 'Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al.', 144], [38, ', 2008a).', 232], [39, 'The location of an argument on syntactic tree provides an intermediate tag for improving the performance.', 77], [40, 'However, building this syntactic tree also introduces the prediction risk inevitably.', 299], [41, 'The analysis in (Pradhan et al.', 220], [42, ', 2005) found that the major source of the incorrect predictions was the syntactic parser.', 163], [43, 'Combination of different syntactic parsers was proposed to address this problem, from both feature level and model level (Surdeanu et al.', 152], [44, ', 2007;', 300], [45, 'Koomen et al.', 301], [46, ', 2005;', 278], [47, 'Pradhan et al.', 281], [48, ', 2005).', 302], [49, 'Besides, feature templates in this classiﬁcation task strongly rely on the expert experience.', 72], [50, 'They need iterative modiﬁcation after analyzing how the system performs on development data.', 132], [51, 'When the corpus and data distribution are changed, or when people move to another language, the feature templates have to be re-designed.', 215], [52, 'To address the above issues, (Collobert et al.', 303], [53, ', 2011) proposed a uniﬁed neural network architecture using word embedding and convolution.', 98], [54, 'They applied their architecture on four standard NLP tasks: Part-Of-Speech tagging (POS), chunking (CHUNK), Named Entity Recognition (NER) and Semantic Role Labeling (SRL).', 113], [55, 'They were able to reach the previous state-of-the-art performance on all these tasks except for SRL.', 96], [56, 'They had to resort to parsing features in order to make the system competitive with state-of-the-art performance.', 99], [57, 'In this work, we propose an end-to-end system using deep bi-directional long short-term memory (DB-LSTM) model to address the above difﬁculties.', 238], [58, 'We take only original text as the input features, without any intermediate tag such as syntactic information.', 304], [59, 'The input features are processed by the following 8 layers of LSTM bidirectionally.', 164], [60, 'At the top locates the conditional random ﬁeld (CRF) model for tag sequence prediction.', 305], [61, 'We achieve the state-of-the-art performance of f-score F1= 81.', 89], [62, '07 on CoNLL-2005 shared task and F1= 81.', 206], [63, '27 on CoNLL-2012 shared task.', 207], [64, 'At last, we ﬁnd the traditional syntactic information can also be inferred from the learned representations.', 257], [65, 'People solve SRL problems in two major ways.', 221], [66, 'The ﬁrst one follows the traditional spirit widely used in NLP basic problems.', 222], [67, 'A linear classiﬁer is employed with feature templates.', 306], [68, 'Most efforts focus on how to extract the feature templates that can best describe the text properties from training corpus.', 188], [69, 'One of the most important features is from syntactic parsing, although syntactic parsing is also considered as a difﬁcult problem.', 11], [70, 'Thus system combination appear to be the general solution.', 307], [71, 'In the work of (Pradhan et al.', 125], [72, ', 2005), the syntactic tags are produced by Charniak parser (Charniak, 2000;', 308], [73, 'Charniak and Johnson, 2005) and Collins parser (Collins, 2003) respectively.', 309], [74, 'Based on this, different systems are built to generate SRL tags.', 208], [75, 'These SRL tags are used to extend the original feature templates, along with ﬂat syntactic chunking results.', 310], [76, 'At last another classiﬁer learns the ﬁnal SRL tag from the above results.', 258], [77, 'In their analysis, the combination of three different syntactic view brings large improvement for the system.', 126], [78, 'Similarly, Koomen et al.', 311], [79, '(Koomen et al.', 312], [80, ', 2005) combined the system in another way.', 189], [81, 'They built multiple classiﬁers and then all outputs are combined through an optimization problem.', 313], [82, 'Surdeanu et al.', 292], [83, 'fully discussed the combination strategy in (Surdeanu et al.', 190], [84, ', 2007).', 314], [85, 'Beyond the above traditional methods, the second way try to solve this problem without feature engineering.', 315], [86, 'Collobert et al.', 316], [87, '(Collobert et al.', 317], [88, ', 2011) introduced a neural network model consists of word embedding layer, convolution layers and CRF layer.', 33], [89, 'This pipeline addressed the data sparsity by initializing the model with word embeddings which is trained from large unlabeled text corpus.', 259], [90, 'However, the convolution layer is not the best way to model long distance dependency since it only includes words within limited context.', 318], [91, 'So they processed the whole sequence for each given pair of argument and predicate.', 165], [92, 'This results in the computational complexity of O(npL2), with L denoting the sequence length and npthe number of predicate, while the complexity of our model is linear (O(npL)).', 22], [93, 'Moreover, in order to catch up with the performance of traditional methods, they had to incorporate the syntactic features by using parse trees of Charniak parser (Charniak, 2000) which still provides the major contribution.', 47], [94, 'At the inference stage, structural constraints often lead to improved results (Punyakanok et al.', 319], [95, ', 2008b).', 320], [96, 'The constraints comes from annotation conventions of the task and other linguistic considerations.', 153], [97, 'With dynamic programming, (T¨ackstr¨om et al.', 321], [98, ', 2015) enhance the inference efﬁciency further.', 322], [99, 'But designation of the constraints depends much on the linguistic knowledge.', 53], [100, 'Nevertheless, the attempts of building end-toend systems for NLP become popular in recent years.', 100], [101, 'Inspired by the work in computer vision, people hierarchically organized a window of words through convolution layers in deep form to account for the higher level of organization to solve the document classiﬁcation task (Kim, 2014;', 4], [102, 'Zhang and LeCun, 2015).', 323], [103, 'Step further, people have also achieved success in directly mapping the sequence to sequence level target as the work in dependency parsing and machine translation (Vinyals et al.', 70], [104, ', 2014;', 324], [105, 'Sutskever et al.', 325], [106, ', 2014).', 326], [107, 'In this paper, we propose an end-to-end system based on recurrent topology.', 143], [108, 'Recurrent neural network (RNN) has natural advantage in modeling sequence problems.', 191], [109, 'The past information is built up through the recurrent layer when model consumes the sequence word by word as shown in Eq.', 192], [110, '1. x and y are the input and output of the recurrent layer with (t) denoting the time step, wmfand wmiare the matrix from input or recurrent layer to hidden layer.', 154], [111, 'σ is the activation function.', 327], [112, 'Without y(t−1)term, the rnn model returns to the feed forward form.', 328], [113, 'However, people often met with two difﬁculties.', 329], [114, 'First, information of the current word strongly depends on distant words, rather than its neighborhood.', 54], [115, 'Second, gradient parameters may explode or vanish especially in processing long sequences (Bengio et al.', 193], [116, ', 1994).', 330], [117, 'Thus long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) was proposed to address the above difﬁculties.', 331], [118, 'In the following part, we will ﬁrst give a brief introduction about the LSTM and then demonstrate how to build up a network based on LSTM to solve a typical sequence tagging problem: semantic role labeling.', 5], [119, 'Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997;', 332], [120, 'Graves et al.', 333], [121, ', 2009) is an RNN architecture speciﬁcally designed to address the vanishing gradient and exploding gradient problems.', 334], [122, 'The hidden neural units are replaced by a number of memory blocks.', 63], [123, 'Each memory block contains several cells, whose activations are controlled by three multiplicative gates: the input gate, forget gate and output gate.', 335], [124, 'With the above change, the original rnn model is improved to be: Now y is the memory block output.', 336], [125, 'n is equivalent to the original hidden value y in rnn model.', 194], [126, 'ρ, φ and π are the input, forget and output gates value.', 337], [127, 'sc,mis state value of cell c in block m and c is ﬁxed to be 1 and omitted in common work.', 101], [128, 'The computation of three multiplicative gates comes from input value, recurrent value and cell state value with different activations σ respectively as shown in the following and Fig.', 73], [129, '1:', 338], [130, 'Figure 1: LSTM memory block with a single cell.', 233], [131, '(Graves et al.', 339], [132, ', 2009)', 340], [133, 'The effect of the gates is to allow the cells to store and access information over long periods of time.', 90], [134, 'When the input gate is closed, the new coming input information will not affect the previous cell state.', 341], [135, 'Forget gate is used to remove the historical information stored in the cells.', 195], [136, 'The rest of the network can access the stored value of a cell only when its output gate is open.', 27], [137, 'In language related problems, the structural knowledge can be extracted out by processing sequences both forward and backward so that the complementary information from the past and the future can be integrated for inference.', 161], [138, 'Thus bidirectional LSTM (B-LSTM) containing two hidden layers were proposed(Schuster and Paliwal, 1997).', 342], [139, 'Both hidden layers connect to the same input layer and output layer, processing the same sequence in two directions respectively (A.', 145], [140, 'Graves, 2013).', 343], [141, 'In this work, we utilize the bi-directional information in another way.', 151], [142, 'First a standard LSTM processes the sequence in forward direction.', 88], [143, 'The output of this LSTM layer is taken by the next LSTM layer as input, processed in reversed direction.', 102], [144, 'These two standard LSTM layers compose a pair of LSTM.', 51], [145, 'Then we stack LSTM layers pair after pair to obtain the deep LSTM model.', 344], [146, 'We call this topology as deep bi-directional LSTM (DB-LSTM) network.', 345], [147, 'Our experiments show that this architecture is critical to achieve good performance.', 346], [148, 'We process the sequence word by word.', 347], [149, 'Two input features play an essential role in this pipeline: predicate (pred) and argument (argu), with argument describing the word under processing.', 87], [150, 'The output for this pair of words is their semantic role.', 166], [151, 'If a sequence has nppredicates, we will process this sequence nptimes.', 149], [152, 'We also introduce two other features, predicate context (ctx-p) and region mark (mr).', 348], [153, 'Since a single predicate word can not exactly describe the predicate information, especially when the same words appear more than one times in a sentence.', 7], [154, 'With the expanded context, the ambiguity can be largely eliminated.', 349], [155, 'Similarly, we use region mark mr= 1 to denote the argument position if it locates in the predicate context region, or mr= 0 if not.', 223], [156, 'These four simple features are all we need for our SRL system.', 350], [157, 'In Tab.', 239], [158, '1 we give an example sequence with the labels for each word.', 351], [159, 'We do not use other types of features such as part of speech (POS), syntactic parsing, etc.', 91], [160, 'time argu pred ctx-p mrlabel Table 1: An example sequence with 4 input features: argument, predicate, predicate context (context length is 3) , region mark.', 352], [161, '“IOB” tagging scheme is used (Collobert et al.', 353], [162, ', 2011).', 283], [163, 'Because the large number of parameters associated with the argument words, similar to (Collobert et al.', 167], [164, ', 2011), the pre-trained word representations are employed to address the data sparsity issue.', 354], [165, 'We used a large unlabeled text corpus to train a neural language model (NLM) (Bengio et al.', 23], [166, ', 2006;', 355], [167, 'Bengio et al.', 356], [168, ', 2003) and then initialized the argument and predicate word representations with parameters from the NLM representations.', 260], [169, 'There are various ways of obtaining good word representations (Mikolov et al.', 160], [170, ', 2013;', 357], [171, 'Collobert and Weston, 2008;', 358], [172, 'Mnih and Kavukcuoglu, 2013;', 359], [173, 'Yu et al.', 360], [174, ', 2014).', 326], [175, 'A systematic comparison of them on the task of SRL is beyond the scope of this work.', 13], [176, 'The above four features are concatenated to be the input representation at this time step for the following LSTM layers.', 361], [177, 'As described in Sec.', 224], [178, '3.1, we use DB-LSTM topology to learn the sequence knowledge and we build up to 8 layers of DBLSTM in our work.', 119], [179, 'As in traditional methods, we employ CRF (Lafferty et al.', 225], [180, ', 2001) on top of the network for the ﬁnal prediction.', 78], [181, 'It takes the representations provided by the last LSTM layer as input to model the strong dependance among adjacent tags.', 362], [182, 'Figure 2: DB-LSTM network.', 363], [183, 'Shadow part denote the predicate context within length 1.', 364], [184, 'The complete model with 4 LSTM layers is illustrated in Fig.', 219], [185, '2. At the bottom of the graph locates the word sequence in Tab.', 120], [186, '1. For a given time step (step 2 as an example), argument and predicate are speciﬁed with different color.', 49], [187, 'We use the shadowed region to denote the predicate context.', 365], [188, 'The temporal expanded version of the model is shown in Fig.', 121], [189, '3. L-H denotes the LSTM hidden layer.', 366], [190, 'We use the stochastic gradient descent (SGD) algorithm as the training technique for the whole pipeline (Lecun et al.', 367], [191, ', 1998).', 368], [192, 'For a given sequence, we look up the embedding of each word and process this vector with the following LSTM layers for the high level representation.', 46], [193, 'After having ﬁnished the whole sequence, we take the representations of all time steps as the input features Figure 3: Temporal expanded DB-LSTM network.', 168], [194, 'Bars denote that the connections are blocked by the closed gates.', 369], [195, 'Shadow part denotes the predicate context.', 370], [196, 'for CRF to perform the sequence tagging task.', 371], [197, 'The traditional viterbi decoding is used for inference.', 372], [198, 'The gradient of the log-likelihood of the tag sequence with respect to the input of the CRF is calculated and back-propagated to all the DB-LSTM layers to get the gradient of the parameters (Collobert et al.', 26], [199, ', 2011).', 283], [200, 'We mainly evaluated and analyzed our system on the commonly used CoNLL-2005 shared task data set and the conclusions are also validated on CoNLL-2012 shared task.', 38], [201, 'CoNLL-2005 data set takes section 2-21 of Wall Street Journal (WSJ) data as training set, and section 24 as development set.', 104], [202, 'The test set consists of section 23 of WSJ concatenated with 3 sections from Brown corpus (Carreras and M`arquez, 2005).', 71], [203, 'CoNLL-2012 data set is extracted from OntoNotes v5.', 235], [204, '0 corpus.', 373], [205, 'The description and separation of train, development and test data set can be found in (Pradhan et al.', 85], [206, ', 2013).', 374], [207, 'We trained word embeddings with English Wikipedia (Ewk) corpus using NLM (Bengio et al.', 375], [208, ', 2006).', 376], [209, 'The corpus contains 995 million tokens.', 377], [210, 'We transformed all the words into their lowercase and the vocabulary size is 4.', 378], [211, '9 million.', 379], [212, 'About 5% words in CoNLL 2005 data set can not be found in Ewk dictionary and are marked as .', 159], [213, 'In all experiments, we use the same word embedding with dimension 32.', 213], [214, 'In this part, we will analyze the performance of two different networks, the CNN and LSTM network.', 127], [215, 'Although at last we ﬁnd CNN can not provide the results as good as that from LSTM, the analysis still help us to gain a deep insight of this problem.', 12], [216, 'In CNN, we add argument context as the ﬁfth feature and the other four features are the same as that used in LSTM.', 201], [217, 'In order to have good understanding of the contribution from each modeling decision, we started from a simple model and add more units step by step.', 19], [218, 'Using CNN to solve SRL problem has been introduced in (Collobert et al.', 146], [219, ', 2011).', 283], [220, 'Since we only focus on the analysis of features, a simpliﬁed version is used here.', 18], [221, 'Our feature set consists of ﬁve parts as described above.', 138], [222, 'The representation of argument and predicate can be obtained by looking up the Emb(Ewk) dictionary.', 169], [223, 'And the representation of argument context and predicate context can be obtained by concatenating the embedding of each word in the context.', 30], [224, 'For each of the above four parts, we add a hidden layer.', 40], [225, 'Then all these four hidden layers together with region mark are projected onto the next hidden layer.', 380], [226, 'At last we use a CRF layer for prediction (See Fig.', 150], [227, '4).', 381], [228, 'With above set up, the computational complexity is O(npL).', 249], [229, 'Figure 4: CNN Pipeline.', 382], [230, 'Shadow parts denote the argument context and predicate context respectively', 383], [231, 'The size of hidden layers connected to argument or predicate is set to be h1w= 32.', 139], [232, 'The size of the other two hidden layers connected to context embedding is set to be h1c= 128 since the corresponding inputs are larger.', 117], [233, 'To simplify the parameter setting and results comparison, we use the same learning rate l = 1 × 10−3for each layer and keep this rate a constant during model training.', 9], [234, 'The second hidden layer dimension h2is also 128.', 384], [235, 'All hidden layer activation function is tanh.', 385], [236, 'In Tab.', 239], [237, '2, it is shown that longer argument and predicate context result in better performance, since longer context brings more information.', 107], [238, 'We observe the same trends in other NLP experiments, such as NER, POS tagging.', 196], [239, 'The difference is that we do not need to use the context length up to 11.', 386], [240, 'This is because most of the useful information for NER and POS tagging is local respect the label position, while in SRL there exists long distance relationship.', 79], [241, 'So in traditional methods for SRL, syntactic trees are often introduced to account for such relation.', 197], [242, 'In order to see whether the improvement from CNN-2 to CNN-3 is due to longer context or larger model size, we tested a model CNN6 with same context length but more model parameters.', 66], [243, 'As we can see from the result of CoNLL2005 data set (Tab.', 122], [244, '2), larger model does not improve the result.', 387], [245, 'Table 2: F1of CNN method on development set and test set of CoNLL-2005 data set.', 41], [246, 'Without using region mark (mr) feature, the F1 drops from the 53.', 261], [247, '07 of CNN-3 to the 37.', 170], [248, '50 of CNN-5.', 171], [249, 'Since it is generally believed that words near the predicate are more likely to be related to the predicate.', 388], [250, 'SRL is a typical problem with long distance dependency, while the convolution operation can only learn the knowledge from the limited neighborhood.', 64], [251, 'This is why we have to introduce long context.', 389], [252, 'However, the language information can not be expressed just by linearly expanding the context as what we did in CNN pipeline.', 67], [253, 'In order to better summarize the sequence structure, we turn to LSTM network.', 240], [254, 'Here the feature set consists of four parts.', 140], [255, 'Argument and predicate are necessary parts in this problem.', 226], [256, 'In recurrent model, argument context (ctx-a) is no longer needed and we only expand the predicate context.', 75], [257, 'We also need the region mark deﬁned in the same way as in CNN.', 198], [258, 'The architecture has been shown in Fig.', 227], [259, '2 and described in Sec.', 228], [260, '3.2.', 390], [261, 'Since it is difﬁcult to propagate the error from the top to the bottom layers, we use two learning rates.', 262], [262, 'At the bottom, i.', 391], [263, 'e. from embeddings to the ﬁrst LSTM layer, we use lb= 1 × 10−2for model depth d <= 4 and lb= 2 × 10−2for d > 4.', 263], [264, 'For the other LSTM layers and CRF layer, we set learning rate l = lb× 10−3.', 250], [265, 'We kept all learning rates constant during training.', 392], [266, 'The model size can be enlarged by increasing the number of LSTM layers (d) or the dimension of hidden layers (h).', 92], [267, 'L2 weight decay in SGD is used for model regularization and we set its strength r2= 8 × 10−4: where w denotes the parameter, g the gradient of the log likelihood of the label with respect to the parameter.', 61], [268, 'We started on CoNLL-2005 dataset from a small model with only one LSTM layer and h = 32.', 14], [269, 'All word embeddings were randomly initialized.', 393], [270, 'Predicate context length was 1.', 394], [271, 'Region mark is not used.', 395], [272, 'With this model, we obtained F1= 49.', 396], [273, '44 (Tab.', 397], [274, '3), better than that of CNN without using argument context (41.', 172], [275, '24) or region mark (37.', 398], [276, '50).', 399], [277, 'This result suggests that, the recurrent structure can extract sequential information more effectively than CNN.', 400], [278, 'By adding predicate context with length 5, F1 is improved from 49.', 264], [279, '44 to 56.', 401], [280, '85 (Tab.', 402], [281, '3).', 403], [282, 'This is because we only recurrently process the argument word, so we still need predicate context for more detail.', 404], [283, 'Further more, F1rises to 58.', 405], [284, '71 with region mark feature.', 406], [285, 'The reason is the same as we explained in CNN pipeline.', 147], [286, 'Next we change the random initialization of word representation to the pre-trained word representation from Emb(Ewk).', 155], [287, 'This representation is ﬁxed in the training process.', 148], [288, 'F1rises to 65.', 407], [289, '11 (See Tab.', 408], [290, '3).', 403], [291, 'So far, we have shown the effect from each part of features in LSTM network.', 111], [292, 'The conclusion is consistent with what we found in CNN network.', 229], [293, 'Besides, LSTM exhibits better abilities to learn the sequence structure.', 409], [294, 'Next, we gradually increase the model size to further enhance the performance.', 410], [295, 'CoNLL-2005 data set', 251], [296, 'CoNLL-2012 data set Table 3: F1with LSTM method on development set and test set of CoNLL-2005 data set and CoNLL-2012 data set.', 57], [297, 'Emb: the type of embedding.', 135], [298, 'd: the number of LSTM layers.', 173], [299, 'ctx-p: predicate context length.', 411], [300, 'mr: region mark feature.', 412], [301, 'h: hidden layer size.', 413], [302, 'We ﬁnd that the critical improvement comes from increasing the depth of LSTM network.', 156], [303, 'After adding a reversed LSTM layer, F1is improved from 65.', 131], [304, '11 to 72.', 414], [305, '56.', 415], [306, 'And the F1of the system with d = 4, 6, 8 are 75.', 416], [307, '74, 78.', 417], [308, '02 and 78.', 418], [309, '28 respectively.', 419], [310, 'With 6-layer network, we have outperformed the CoNLL-2005 shared task winner system with F1= 77.', 420], [311, '92 (Koomen et al.', 421], [312, ', 2005).', 302], [313, 'Our experiment results also show that the further performance gain by increasing the depth from 6 to 8 is relative small.', 265], [314, 'Another way to increase the model size is to increase the hidden layer dimension h.', 422], [315, 'We gradually increase the dimension from 32 to 64, 128, and the corresponding results are listed in Tab.', 124], [316, '3. The best F1we obtained is 80.', 423], [317, '28 with h = 128.', 424], [318, 'We also show the result F1= 80.', 425], [319, '70 on CoNLL-2012 dataset in Tab.', 158], [320, '3 with exactly the same setup.', 426], [321, 'In the above experiments, learning rate and weight decay rate are ﬁxed for the sake of simplicity in comparing different models.', 56], [322, 'To further improve the model, we perform a ﬁne tuning step to adjust the parameters based on previously trained model.', 45], [323, 'This includes the relaxation of weight decay and decrease of learning rate.', 93], [324, 'We set r2= 4 × 10−4and lb= 1 × 10−2, and obtain F1= 81.', 252], [325, '07 as the ﬁnal result of CoNLL-2005 data set and F1= 81.', 141], [326, '27 of CoNLL-2012 data set.', 142], [327, 'CoNLL-2005 dev test WSJ Brown', 427], [328, 'Koomen (single parser) 74.', 428], [329, '76 - - -', 429], [330, 'T¨ackstr¨om 78.', 430], [331, '6 - 79.', 431], [332, '9 71.', 432], [333, 'CoNLL-2012 dev test - -', 433], [334, 'T¨ackstr¨om 79.', 434], [335, '1 79.', 435], [336, 'Table 4: Comparison with previous methods.', 436], [337, 'In Tab.', 239], [338, '4, we compare the performance of other works.', 174], [339, 'On CoNLL-2005 shared task, merging syntactic tree at feature level instead of model level exhibits the similar performance with F1= 77.', 175], [340, '30 (Pradhan et al.', 437], [341, ', 2005).', 302], [342, 'After further investigation on model combination, Surdeanu et al.', 95], [343, 'obtained a better system (Surdeanu et al.', 110], [344, ', 2007).', 314], [345, 'We also list the results from (Toutanova et al.', 266], [346, ', 2008) and (T¨ackstr¨om et al.', 438], [347, ', 2015) of the joint model with additional considerations of standard linguistic assumptions.', 94], [348, 'For convolution based methods (Collobert et al.', 254], [349, ', 2011), the best F1is 76.', 439], [350, '06, in which syntactic parser plays an essential role.', 230], [351, 'The result without using parser drops down to 74.', 440], [352, '15.', 441], [353, 'On Brown set, we observe the better performance from the work of (Surdeanu et al.', 123], [354, ', 2007) and (T¨ackstr¨om et al.', 442], [355, ', 2015).', 443], [356, 'We hypothesize that DBLSTM is a data-driven method that can not performs well on out-domain dataset.', 21], [357, 'On CoNLL-2012 data set, the traditional method gives F1= 75.', 253], [358, '53 (Pradhan et al.', 444], [359, ', 2013) and a dynamic programming algorithm for efﬁcient constrained inference in SRL gives F1= 79.', 25], [360, '4 (T¨ackstr¨om et al.', 445], [361, ', 2015) , both of them also rely on syntax trees.', 106], [362, 'Since the input feature size is much smaller then the traditional sparse feature templates, the inference stage is very efﬁcient that the model can process 6.', 446], [363, '7k tokens per second on average.', 133], [364, 'We analyze our results on CoNLL-2005 data set.', 185], [365, 'First we list the details including the performance on each sub-classes in Tab.', 97], [366, '5. The results of CoNLL-2005 shared task winner system (Koomen et al.', 176], [367, ', 2005) are also shown for comparison.', 447], [368, 'Their Results (Koomen et.', 448], [369, 'al.', 449], [370, ') Results (Ours)', 450], [371, 'Data set P R F1P R F1 Table 5: F1on each sub sets and classes (CoNLL2005).', 218], [372, '(We remove the classes with low statistics.', 451], [373, ') ﬁnal system is the combination of the results of 5 parsing trees from two different parsers.', 82], [374, 'They also reported the scores of each single system on development set and we list the best one of them (dev(s)).', 37], [375, 'We observe the improvement of F1on development set and test set are 2.', 118], [376, '20 and 3.', 452], [377, '15 respectively.', 453], [378, 'For single system, the improvement is 4.', 454], [379, '79 on development set.', 186], [380, 'We also notice that our model show improvement on both WSJ and Brown test set.', 187], [381, 'The advantage of our model is even more signiﬁcant when comparing with the previous effort of end-to-end training of SRL model (Collobert et al.', 50], [382, ', 2011).', 283], [383, 'Without using linguistic features from parse tree, the F1of Collobert’s model is 74.', 267], [384, '15, which is 6.', 455], [385, '92 lower than our model.', 456], [386, 'In order to analyze the performance of our model on the sentences with different lengths, we split the data into 6 bins according to the sentence length, with bin width being 10 words and the last bin includes sequences with L > 50 because of insufﬁcient data for longer sentences.', 31], [387, 'Fig.', 457], [388, '5 shows F1scores at different sequence lengths on WSJ test data and Brown test data for our model and Koomen’s model (baseline) (Koomen et al.', 209], [389, ', 2005).', 302], [390, 'In all curves, performance degrades with increased sentence length.', 241], [391, 'However, the performance gain of our model over the baseline model is larger for longer sentences.', 177], [392, 'Figure 7: Averaged Forget gates value vs.', 458], [393, 'Syntactic distance (CoNLL-2005).', 459], [394, 'The last point includes instances with syntactic distance ds≥ 6.', 460], [395, 'Since we do not use any syntactic information as input feature, we are curious about whether this information can be extracted out from the system parameters.', 268], [396, 'In LSTM, forget gates are used to control the use of historical information.', 128], [397, 'We compute the average value vfgof forget gates of the 7thLSTM layer at word position for a given sentence.', 15], [398, 'We also introduce a variable named syntactic distance dsto represent the number of edges between argument word and predicate word in the dependency parsing tree.', 6], [399, 'Four example sentences are shown in Fig.', 231], [400, '6. For each ﬁgure, the bottom axis denotes an example sentence.', 461], [401, 'At the top of each graph is the corresponding dependency tree built from gold dependency parsing tag.', 157], [402, 'At the bottom, vfgand dsare shown in black and red line.', 199], [403, 'Noticed that the higher forget gates values means “Remember” and smaller values “Forget”.', 462], [404, 'Smaller dsmeans that it is easy to make prediction that long history is unnecessary.', 463], [405, 'On the contrary, large dsresults in a difﬁcult prediction that long historical information is needed.', 48], [406, 'We also computed the average vfgover instances and found it monotonously increases with ds(Fig.', 464], [407, '7).', 465], [408, 'The coincidence of vfgand dssuggests that the model implicitly captures some syntactic structure.', 178], [409, 'Figure 6: Forget gates value vs.', 466], [410, 'Syntactic distance on four example sentences.', 210], [411, 'Top: dependency parsing tree from gold tag.', 269], [412, 'Green square word: predicate word.', 467], [413, 'Bottom black solid lines: forget gates value at each time step.', 468], [414, 'Bottom red empty square lines: gold syntactic distance between the current argument and predicate.', 469], [415, 'able to bypass the traditional steps for extracting the intermediate NLP features such as POS and syntactic parsing and avoid human engineering the feature templates.', 470], [416, 'The model is trained to predict the SRL tag directly from the original word sequence with four simple features without any explicit linguistic knowledge.', 270], [417, 'Our model achieves F1score of 81.', 179], [418, '07 on CoNLL-2005 shared task and 81.', 211], [419, '27 on CoNLL-2012 shared task, both outperforming the previous systems based on parsing results and feature engineering, which heavily rely on the linguistic knowledge from expert.', 76], [420, 'Furthermore, the simpliﬁed feature templates results in high inference efﬁciency with 6.', 200], [421, '7k tokens per second.', 287], [422, 'In our experiments, increasing the model depth is the major contribution to the ﬁnal improvement.', 242], [423, 'With deep model, we achieve strong ability of learning semantic rules without worrying about over-ﬁtting even on such limited training set.', 65], [424, 'It also outperforms the convolution method with large context length.', 471], [425, 'Moreover, with more sophisticatedly designed network and training technique based on LSTM, such as the attempt to integrate the parse tree concept into LSTM framework (Tai et al.', 114], [426, ', 2015), we believe the better performance can be achieved.', 472], [427, 'We show in our analysis that for long sequences our model has even larger advantage over the traditional models.', 204], [428, 'On one hand, LSTM network is capable of capturing the long distance dependency especially in its deep form.', 103], [429, 'On the other hand, the traditional feature templates are only good at describing the properties in neighborhood and a small mistake in syntactic tree will results in large deviation in SRL tagging.', 1], [430, 'Moreover, from the analysis of the internal states of the deep network, we see that the model implicitly learn to capture some syntactic structure similar to the dependency parsing tree.', 83], [431, 'It is encouraging to see that deep learning models with end-to-end training can outperform traditional models on tasks which are previously believed to heavily depend on syntactic parsing (Koomen et al.', 59], [432, ', 2005;', 278], [433, 'Pradhan et al.', 281], [434, ', 2013).', 374], [435, 'However, we recognize that semantic role labeling itself is an intermediate step towards the language problems we really care about, such as question answering, information extraction etc.', 214], [436, 'We believe that end-to-end training with some suitable deep structure yet to be invented might be proven to be effective to solving these problems.', 473], [437, 'And we are seeing some recent active research exploring this possibility (Weston et al.', 474], [438, ', 2014;', 324], [439, 'Weston et al.', 475], [440, ', 2015;', 476], [441, 'Graves et al.', 333], [442, ', 2014).', 326], [443, 'G. Hinton A.', 477], [444, 'Graves, A.', 478], [445, 'Mohamed.', 479], [446, '2013.', 480], [447, 'Speech recognition with deep recurrent neural networks.', 481], [448, 'In IEEE International Conference on Acoustics,Shen Dan and Mirella Lapata.', 60], [449, '2007.', 482], [450, 'Using semanSpeech, and Signal Processing, ICASSP 2013.', 256], [451, 'Emanuele Bastianelli, Giuseppe Castellucci, Danilo Croce, and Roberto Basili.', 483], [452, '2013.', 480], [453, 'Textual inference and meaning representation in human robot interaction.', 108], [454, 'In Proceedings of the Joint Symposium on Semantic Processing.', 81], [455, 'Textual Inference and StructuresAlex Graves, Marcus Liwicki, Santiago Fernanin Corpora, pages 65–69.', 484], [456, 'Yoshua Bengio, Patrice Simard, and Paolo Frasconi.', 485], [457, '1994.', 486], [458, 'Learning long-term dependencies with gradient descent is difﬁcult.', 247], [459, 'IEEE Transactions on Neural Networks, 5(2):157–166.', 112], [460, 'Yoshua Bengio, R', 487], [461, '´ejean Ducharme, Pascal Vincent, andAlex Graves, Greg Wayne, and Ivo Danihelka.', 488], [462, '2014.', 489], [463, 'Christian Janvin.', 490], [464, '2003.', 491], [465, 'A neural probabilistic language model.', 216], [466, 'Journal of Machine Learning Re-S.', 136], [467, 'Hochreiter and J.', 492], [468, 'J search, 3:1137–1155, March.', 493], [469, 'Yoshua Bengio, Holger Schwenk, Jean-Sbastien Sencal, Frderic Morin, and Jean-Luc Gauvain.', 494], [470, '2006.', 495], [471, 'Yoon Kim.', 496], [472, '2014.', 489], [473, 'Convolutional neural networks for Neural probabilistic language models.', 183], [474, 'In Innovations in Machine Learning, volume 194 of Studies in Fuzziness and Soft Computing, pages 137–186.', 10], [475, 'Springer Berlin Heidelberg.', 497], [476, 'Xavier Carreras and Llu', 498], [477, '´ıs M`arquez.', 499], [478, '2005.', 500], [479, 'Intro-Peter Koomen, Vasin Punyakanok, Dan Roth, and duction to the CoNLL-2005 shared task: Semantic role labeling.', 501], [480, 'In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 152–164, Ann Arbor, Michigan, June.', 34], [481, 'Association for Computational Linguistics.', 502], [482, 'Eugene Charniak and Mark Johnson.', 503], [483, '2005.', 500], [484, 'Coarseto-ﬁne n-best parsing and maxent discriminative reranking.', 504], [485, 'In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 173–180, Stroudsburg, PA, USA.', 42], [486, 'Association for Computational Linguistics.', 502], [487, 'Eugene Charniak.', 505], [488, '2000.', 506], [489, 'A maximum-entropyinspired parser.', 507], [490, 'In Proceedings of the 1st NorthYann Lecun, Lon Bottou, Yoshua Bengio, and Patrick American Chapter of the Association for Computational Linguistics Conference, NAACL 2000, pages 132–139, Stroudsburg, PA, USA.', 74], [491, 'Association for Computational Linguistics.', 502], [492, 'Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-', 276], [493, 'Michael Collins.', 508], [494, '2003.', 491], [495, 'Head-driven statistical models for natural language parsing.', 203], [496, 'Comput.', 509], [497, 'Linguist.', 510], [498, ', 29(4):589–637, December.', 511], [499, 'Ronan Collobert and Jason Weston.', 512], [500, '2008.', 513], [501, 'A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning.', 217], [502, 'In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 160–167, New York, NY, USA.', 28], [503, 'ACM.', 514], [504, 'Alessandro Moschitti, Paul Morarescu, and Sanda M.', 515], [505, 'Ronan Collobert, Jason Weston, L', 516], [506, '´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.', 517], [507, '2011.', 518], [508, 'Natural language processing (almost) from scratch.', 202], [509, 'Journal of Marchine Learning Research, 12:2493–2537, November.', 137], [510, 'tic roles to improve question answering.', 274], [511, 'In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).', 8], [512, 'dez, Roman Bertolami, Horst Bunke, and J¨urgen Schmidhuber.', 519], [513, '2009.', 520], [514, 'A novel connectionist system for unconstrained handwriting recognition.', 521], [515, 'IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(5):855–868.', 134], [516, 'Long short-term memory.', 522], [517, 'Neural Computation, 9(8):1735–1780.', 246], [518, 'sentence classiﬁcation.', 523], [519, 'In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1746–1751.', 24], [520, 'Wen-tau Yih.', 524], [521, '2005.', 500], [522, 'Generalized inference with multiple semantic role labeling systems.', 525], [523, 'In Proceedings of the 9th Conference on Computational Natural Language Learning, CONLL ’05, pages 181–184, Stroudsburg, PA, USA.', 35], [524, 'Association for Computational Linguistics.', 502], [525, 'John D.', 526], [526, 'Lafferty, Andrew McCallum, and Fernando C.', 527], [527, 'N. Pereira.', 528], [528, '2001.', 529], [529, 'Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data.', 245], [530, 'In Proceedings of the 8th International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA.', 29], [531, 'Morgan Kaufmann Publishers Inc.', 530], [532, 'Haffner.', 531], [533, '1998.', 532], [534, 'Gradient-based learning applied to document recognition.', 255], [535, 'In Proceedings of the IEEE, pages 2278–2324.', 129], [536, 'rado, and Jeffrey Dean.', 533], [537, '2013.', 480], [538, 'Distributed representations of phrases and their compositionality.', 180], [539, 'In Advances on Neural Information Processing Systems.', 55], [540, 'Andriy Mnih and Koray Kavukcuoglu.', 534], [541, '2013.', 480], [542, 'Learning word embeddings efﬁciently with noise-contrastive estimation.', 248], [543, 'In Advances in Neural Information Processing Systems, pages 2265–2273.', 69], [544, 'Harabagiu.', 535], [545, '2003.', 491], [546, 'Open domain information extraction via automatic semantic labeling.', 536], [547, 'In FLAIRS Conference’03, pages 397–401.', 243], [548, 'Martha Palmer, Daniel Gildea, and Nianwen Xue.', 537], [549, '2010.', 538], [550, 'Semantic Role Labeling.', 539], [551, 'Synthesis Lectures on Human Language Technology Series.', 212], [552, 'Morgan and Claypool.', 540], [553, 'Jacob Persson, Richard Johansson, and Pierre Nugues.', 541], [554, '2009.', 520], [555, 'Text categorization using predicatecargument structures.', 542], [556, 'In Proceedings of NODALIDA, pages 142–149.', 130], [557, 'Kristina Toutanova, Aria Haghighi, and Christopher D.', 543], [558, 'Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.', 544], [559, 'Martin, and Daniel Jurafsky.', 545], [560, '2005.', 500], [561, 'Semantic role chunking combining complementaryOriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, syntactic views.', 546], [562, 'In Proceedings of the 9th Conference on Computational Natural Language Learning, CONLL ’05, pages 217–220, Stroudsburg, PA, USA.', 36], [563, 'Association for Computational Linguistics.', 502], [564, 'Jason Weston, Sumit Chopra, and Antoine Bordes.', 547], [565, 'Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj¨orkelund, Olga Uryupina,Jason Weston, Antoine Bordes, Sumit Chopra, and Yuchen Zhang, and Zhi Zhong.', 548], [566, '2013.', 480], [567, 'Towards robust linguistic analysis using ontonotes.', 275], [568, 'In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143–152, Soﬁa, Bulgaria, August.', 3], [569, 'Association for Computa-Mo Yu, Matthew Gormley, and Mark Dredze.', 549], [570, '2014.', 489], [571, 'tional Linguistics.', 550], [572, 'V. Punyakanok, D.', 551], [573, 'Roth, and W.', 552], [574, 'Yih.', 553], [575, '2008a.', 234], [576, 'The importance of syntactic parsing and inference in se-Xiang Zhang and Yann LeCun.', 80], [577, '2015.', 554], [578, 'Text understandmantic role labeling.', 555], [579, 'Computational Linguistics, 34(2).', 556], [580, 'Vasin Punyakanok, Dan Roth, and Wen tau Yih.', 557], [581, '2008b.', 558], [582, 'The importance of syntactic parsing and inference in semantic role labeling.', 62], [583, 'Computational linguistics, 6(9).', 559], [584, 'M. Schuster and K.', 560], [585, 'K. Paliwal.', 561], [586, '1997.', 562], [587, 'Bidirectional recurrent neural networks.', 563], [588, 'IEEE Transactions on Signal Processing, 45:2673–2681.', 115], [589, 'Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth.', 564], [590, '2003.', 491], [591, 'Using predicate-argument structures for information extraction.', 565], [592, 'In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 8–15, Stroudsburg, PA, USA.', 43], [593, 'Association for Computational Linguistics.', 502], [594, 'Mihai Surdeanu, Llu', 566], [595, '´ıs M`arquez, Xavier Carreras, and Pere R.', 567], [596, 'Comas.', 568], [597, '2007.', 482], [598, 'Combination strategies for semantic role labeling.', 569], [599, 'Journal of Artiﬁcial Intelligence Research, 29:105–151.', 181], [600, 'Ilya Sutskever, Oriol Vinyals, and Quoc V Le.', 570], [601, '2014.', 489], [602, 'Sequence to sequence learning with neural networks.', 571], [603, 'In Advances on Neural Information Processing Systems.', 55], [604, '¨ackstr¨om, Kuzman Ganchev, and Dipanjan Das.', 572], [605, '2015.', 554], [606, 'Efﬁcient inference and structured learning for semantic role labeling.', 573], [607, 'Transactions of the Association for Computational Linguistics, 3:29–41.', 182], [608, 'Kai Sheng Tai, Richard Socher, and Christopher D.', 574], [609, 'Manning.', 575], [610, '2015.', 554], [611, 'Improved semantic representations from tree-structured long short-term memory networks.', 271], [612, 'In Proceedings of the 53st Annual Meeting on Association for Computational Linguistics, ACL ’15, Stroudsburg, PA, USA.', 44], [613, 'Association for Computational Linguistics.', 502], [614, 'Manning.', 575], [615, '2008.', 513], [616, 'A global joint model for semantic role labeling.', 576], [617, 'Computational Linguistics, 34:161– 191.', 577], [618, 'Ilya Sutskever, and Geoffrey Hinton.', 578], [619, '2014.', 489], [620, 'Grammar as a foreign language.', 52], [621, 'arXiv:1412.', 272], [622, '7449.', 579], [623, 'Tomas Mikolov.', 277], [624, '2015.', 554], [625, 'Towards ai-complete question answering: A set of prerequisite toy tasks.', 86], [626, 'arXiv:1502.', 273], [627, '05698.', 580], [628, 'Factor-based compositional embedding models.', 184], [629, 'In Advances in Neural Information Processing Systems Workshop on Learning Semantics.', 16], [630, 'ing from scratch.', 236], [631, 'arXiv:1502.', 273], [632, '01710.', 581]]