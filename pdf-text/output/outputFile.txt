Like many other tasks involving neural networks, Speech Recognition models are vulnerable to adversarial attacks. However recent research has pointed out differences between attacks and defenses on ASR models compared to image models. Improving the robustness of ASR models requires a paradigm shift from evaluating attacks on one or a few models to a systemic approach in evaluation. We lay the ground for such research by evaluating on various architectures a representative set of adversarial attacks: targeted and untargeted, optimization and speech processing-based, white-box, black-box and targeted attacks. Our results show that the relative strengths of different attack algorithms vary considerably when changing the model architecture, and that the results of some attacks are not to be blindly trusted. They also indicate that training choices such as self-supervised pretraining can signiﬁcantly impact robustness by enabling transferable perturbations. We release our source code as a package that should help future research in evaluating their attacks and defenses. Index Terms: speech recognition, security, robustness, adversarial attacks
---------------------
In recent years Automatic Speech Recognition (ASR) systems have invested an considerable number of practical applications, and security is a critical component of many of them. Yet there is a dangerous lack of certainty regarding the robustness of ASR models to security threats. For instance neural Speech Recognition systems are vulnerable to adversarial attacks, one of the most emblematic shortcomings of modern Deep Learning architectures [1, 2]. These attacks lead models to make wrong predictions by crafting small, hardly perceptible perturbations of their inputs. Applied to domains like banking[3] or home security[4], where ASR systems are used, such attacks could lead to critical failures and potentially enable fraudulent transactions or theft.
---------------------
Adversarial attacks against ASR models have been around for several years and their results on baselines like DeepSpeech2 [5] or Speech Commands [6] are well known [7, 8, 9]. Research on Speech Recognition has since then hit multiple milestones by adopting Transformer models, looking into various architectures and even self-supervision. Yet most works on ASR attacks still often evaluate them on the same baseline models or a very limited number of models [10, 11, 12]. This leaves a number of fundamental questions largely unanswered: how do LSTMs and Transformers compare in terms of robustness? What about encoder-decoders and encoder-only models? Does self-supervised pretraining, a staple of recent ASR systems, affect adversarial vulnerabilities? Robustness for Computer Vision has seen tremendous progress in recent years, largely thanks to standardized benchmarks[13, 14] and baselines [15, 16]. At the same time Randomized Smoothing is the only strong, principled defense that has been adapted to Speech
---------------------
Recognition [17]. Recent works [18, 19] have shown that ASR adversarial examples are different from image ones in key aspects, and must therefore be the object of speciﬁc research. In that regard, setting the foundations for a standardized, systematic evaluation of attacks on speech models is an important ﬁrst step.
---------------------
Our contribution throughout this paper consists in laying out some of these foundations. We start by reproducing a representative panel of attacks: targeted and untargeted, with and without labels, white-box and black-box, optimization and signal processing-based. We evaluate them on the LibriSpeech test-clean set against multiple standard models trained on the same data but varying in architecture, loss, training paradigm and size. These results both set a standard to beat in future attack and defense works, and let us draw some (cautious) conclusions on the relative robustness of ASR architectures.
---------------------
We also investigate the conditions under which adversarial examples may transfer between models. We study multiple architectures - notably variations of Wav2Vec2 that were pretrained with self-supervised objectives. Surprisingly, we show that such pretraining signiﬁcantly boosts the transferability of adversarial examples, even for targeted attacks - which so far were systematically non-transferable between ASR models [19]. To an extent, these results indicate that ASR models today are more vulnerable than in the past. We draw attention on the dangers of ﬁne-tuning private models from publicly available speech embeddings.
---------------------
Arguably one of the limitations for a standard evaluation of ASR attacks until this point was the lack of a practical codebase to do so. Our ﬁnal contribution consists in releasing our source codein the form of an simply extensible package built on the Speechbrain toolkit [20]. We hope this will help performing a thorough evaluation of adversarial attacks in the future.
---------------------
In this section we describe the attacks that we evaluated. We adopt the taxonomy of [18].
---------------------
The Projected Gradient Descent (PGD) attack is an direct optimization attack. We use the untargeted PGD, which aims at producing denial of service by by generating any wrong output. It optimizes the following objective with projected gradient descent for multiple steps: where f is the model, L its loss function, (x, y) the original input and label, and  a bound on the adversarial perturbation δ. https://github.com/RaphaelOlivier/robust_ speech
---------------------
PGD is a general machine learning attack and has been applied to ASR models in the past [18, 17]. We consider both the L and Lfor the choice of norm in Eq. 1. We control the perturbation size of the latter by ﬁxing the Signal-Noise Ratio (SNR) of the attack as a hyperparameter. The SNR is deﬁned with and measured in decibels (dB). For each input we set  accordingly:  = kxk/10. For LPGD, we directly set  as a hyperparameter.
---------------------
[9] propose an indirect optimization attack for ASR models. It employs a genetic algorithm where a population is maintained and renewed, using the adversarial objective as a ﬁtness score. Such indirect attacks are useful to attack models that are only available as a black-box oracle, without gradient information; but also to fool models whose gradients are not useful due to the obfuscation effect of an incomplete defense [21].
---------------------
The authors of [9] use this method as a targeted attack. However they only evaluated it on the Speech Commands dataset with 10 possible labels. To run such an attack on a large ASR model with a full english sentence as the target is considerably more challenging. To keep some amount of attack success we use the same algorithm for a simpler, untargeted objective: the loss on the original label is used as a ﬁtness score. Perturbations are L-bounded by a hyperparameter .
---------------------
The DFT Kenansville attack [22] is a Signal Processing attack: it does not consider model predictions or loss to modify the audio signal. Instead, this attack removes the spectral components of the input that have the lowest power spectral density (PSD). This leaves human perception mostly unchanged, but typically affects the predictions of ASR models, which tend to exploit high-frequency components more than humans. Like L-PGD the distortion of that attack is controlled with the SNR.
---------------------
The Carlini&Wagner (CW) attack [13] is a targeted, direct optimization attack and a standard for evaluating the robustness of machine learning models. The same authors apply it to the DeepSpeech2 ASR model in [8]. At its core is the optimization of δ to minimize the objective such that kδk< . Here yis the target of the attack and c a regularization parameter controlling the relative importance of the attack objective and the size of the perturbation. The objective is optimized with Adam [23]. The additional Lconstraint  is ﬁrst set large, then decreased as the attack achieves its objective.
---------------------
The Imperceptible attack [11] extends the CW attack by adding a second attack stage where the kδkregularization is replaced with a regularization term based on a psychoacoustic model. This leads to less perceptible adversarial examples. It is also a rare example of attack proposed against a LAS-type model [24] rather than DeepSpeech. We do not report metrics for this attack (they are nearly identical to CW), but we discuss it and provide adversarial examples along with our source code.
---------------------
Recent years have seen the emergence of self-supervised pretraining for ASR models such as Wav2Vec 2.0 [25, 26]. These models are pretrained on unlabeled speech data. During pretraining, given input x the network computes both a quantized latent representation q(x) from low-level features and a contextualized representation c(x) with a Transformer. It is trained to match these two vectors with a contrastive loss.
---------------------
An adversarial example for such a model would intuitively apply a small perturbation δ that makes the representation of c(x + δ) very different from c(x). Even in absence of a training label, attacking self-supervised models is possible [27]. To attack Wav2Vec2 we wrote an algorithm inspired by [28], in which our loss term is simply the squared distance of the natural and adversarial representations kc(x + δ) − c(x)k. By plugging this loss term into the L-PGD objective, we obtain the ”untargeted” SSL attack on Speech.
---------------------
[29] have shown that adversarial examples computed with optimization attacks against a model tend to be effective at fooling other models as well. This transferability effect is a major cause on concern: by attacking a source, proxy model that was trained locally or publicly available, and then transferring the perturbations to the actually targeted model, an adversary can fool even commercial models of unknown architecture.
---------------------
However, transferability is rarely claimed by attacks on ASR models. In fact [18] have shown that it is not achievable with current targeted optimization attacks on the DeepSpeech2 model, even when the models differ only by their random training seed. [19] analyse this phenomenon on a Speech Command model and identify factors limiting transferability, such as recurrent networks or vocabulary size. We extend this analysis with a wider set of architectures and by studying untargeted attacks as well. We also study the effect of self-supervised pretaining on transferability.
---------------------
We use the LibriSpeech dataset [30]. All our models are trained on the full 960h training set, with the exception of the wav2vec2-100h model which was pretrained on the full set and ﬁne-tuned on a 100h clean subset.
---------------------
We evaluate our adversarial attacks on the clean test set. For the most computationally expensive attacks, CW, Imperceptible and Genetic, we restrict our evaluation to 100 utterances only.
---------------------
In order to minimize the time and the computational footprint of our experiments, we evaluate pretrained models directly provided by SpeechBrain [20]. Some models output characters (31 output neurons), others output subwords with 5000 Byte Pair Encodings (BPE). All subwords (resp. character) models share their vocabulary and tokenizer. We do not use external language models for decoding. The detailed hyperparameters of all models are available in our source code.
---------------------
We use two subword Seq2Seq encoder-decoder models following the Listen, Attend and Spell (LAS) architecture [24]. One uses Transformers and and the other LSTMs. Each was trained with both the CTC Loss on the encoded states and Attention+Cross-Entropy Loss. We use only the Cross-Entropy loss and Seq2Seq decoding to run and evaluate attacks on LAS models.
---------------------
By isolating each encoder and applying CTC decoding, we obtain two encoder-only ASR models that achieve good results. We treat these encoders as separate models, sharing their latent speech space with the Seq2Seq models.
---------------------
Finally, we use multiple variations of Wav2Vec 2.0: the base pretrained model, base models trained on 960h and 100h respectively, and the large model trained on 960h. All models were imported using the Speechbrain interface to Huggingface transformers [31].
---------------------
We evaluate all attacks against all models whenever applicable. For the PGD and Kenansville attacks we vary the hyperparameters controlling the amount of noise distortion; however for each attack we keep all other hyperparameters constant. This includes the number of iterations in optimization attacks, which is respectively equal to 100 (PGD), 500 (SSL), 2000 (Genetic) and 5000 (CW). These values are greater or equal to those used by their original authors. Our targeted attacks use a set of three possible target sentences. For each input we select the target the closest length to the true label.
---------------------
To evaluate our attacks we rely on several metrics: • For untargeted attacks, the Word-Error Rate (WER) of the model on the adversarial example with respect to the true label. The higher, the stronger the attack. • For targeted attacks, the WER on the adversarial example with respect to the target sentence. The lower, the stronger the attack. We also report the accuracy, or success rate of these attacks, i.e. the fraction of inputs for which a successful adversarial example was found. • The Signal-Noise Ratio (SNR) of the perturbation: the higher the SNR, the less perceptible the noise. 20dB can be considered a threshold for ”acceptable” noise level that does not disturb human comprehension; 30 − 40dB is an estimate of ”imperceptible” noise. Both these threshold are approximate, as human perception does not align well with Lnorms.
---------------------
Since we vary the bounds of PGD and Kenansville we can, for these attacks, plot the WER as a function of the SNR.
---------------------
We plot the results of the PGD and Kenansville attacks in Figure 1, while the CW and Genetic attack results are displayed in Table 1.
---------------------
We start by analysing direct optimization attacks: PGD (Fig 1a , 1b), and CW (Table 1 columns 2-5).
---------------------
The analysis of the PGD plots show large variations in the WER of different models under attack. The relative ranks of all
---------------------
Table 1: results of all models on clean data and under the (targeted) Carlini&Wagner attack and the (untargeted) Genetic attack. For CW we report the WER, SNR and attack accuracy (i.e. success rate). The bound for the Genetic attack corresponds to an average SNR of 20dB. The up and down arrows indicate whether large or small WER values mean successful attacks. models are consistent between Land Lattacks. The only exception is the Transformer CTC encoder, which compared to other models is more robust to Lattacks than Lattacks. CW results on the other hand show a clear gap between the Transformer models and the LSTM models: the former always predict the attack target on adversarial examples with low noise (SNR ≥ 30dB), while the latter require more noise for imperfect target transcriptions. The Imperceptible attack achieves almost identical results (not reported), with SNRs varying by up to 3dB.
---------------------
Overall two trends emerge from these results:
---------------------
• Encoders-decoder are harder to attack than encoder-only models. LAS models get lower WER than CTC models on PGD and lower SNR on CW attacks. The Transformer LAS model has a lower PGD WER than the Transformer encoder.
---------------------
PGD, but easier with CW.
---------------------
The higher robustness of encoder-decoders may simply be a result of their higher depth, which is a known cause of gradient obfuscation [21]. The fact that CW is effective against Transformer Seq2Seq models points towards that explanation: this attack’s objective is especially powerful at circumventing optimization issues. The inconsistent results of PGD and CW against LSTMs and Transformers are harder to explain, and certainly deserve more attention in future works. At the very least it shows that the performance boost of transformers does not clearly carry over in terms of robustness. It also emphasizes the need to evaluate multiple models when crafting new attacks, and multiple attacks when building new models.
---------------------
The Kenansville DFT attack (Figure 1c) has little effect in our evaluation setting: SNRs greater than 20dB fail to increase the WER signiﬁcantly, and adversarial examples with lower SNRs are easily detected by the human ear. Comparing these results to [22] is not easy for two reasons: the authors report different metrics to measure distortion, and evaluate on a different dataset. The adversarial examples they publicly released show that the original speech inputs are of lower quality than the LibriSpeech clean test set. It is therefore possible that Kenansville is most effective on inputs that are already challenging to the ASR models.
---------------------
(a) WER as a function of SNR for the L-(b) WER as a function of SNR for the L PGD attackPGD attack
---------------------
Figure 1: Attacks results for the untargeted (a) L-PGD attack, (b) L values. The legend on the right is shared by all plots.
---------------------
The Genetic attack [9] (Table 1 column 6) also only achieves to degrade the WER of each model by 8 to 20 points, even with an SNR of 20dB. This is not particularly surprising: this attack was originally evaluated on the much less challenging Speech Commands dataset.
---------------------
Table 2: Transferred attack results on selected source/target model pairs. The L-PGD SNR bound is 25dB. Results in italic indicate full or partial attack transferability.
---------------------
Target model WER↑ (30dB) WER↑ (20dB)
---------------------
Table 3: SSL attack results on all models, with SNR bounds of 30dB and 20dB. Attacks are transferred from the Wav2Vec2base pretrained model. Results in italic indicate full or partial attack transferability.
---------------------
In Table 2 we report the results of transferred PGD and CW attacks between selected source-target model pairs.
---------------------
The ﬁrst three rows report our results on supervised LAS and CTC models. They conﬁrm the results that [18] achieved on DeepSpeech2: the CW attack displays almost no transferability between ASR models (WER ≥ 90%), for example between Transformer encoder and LSTM Seq2Seq. Untargeted PGD attacks achieve only a partial degradation of target model’s per-
---------------------
-PGD attack and (c) DFT Kenansville attack, for different SNR formance, even at 25dB SNR. The only exception is the PGD transfer between LSTM CTC and LAS models, i.e. models that share the same encoder. And even such parameter sharing is not sufﬁcient to transfer CW perturbations (WER ≥ 85%).
---------------------
However, results are different between Wav2Vec2 models. When transferring perturbations between Wav2Vec2 base models ﬁne-tuned from the same pretrained encoder, we achieve a PGD degradation to 80% WER, and even partial transferability for CW (48% and 61.11%). These results are conﬁrmed by the SSL attack (Table 3): perturbations crafted against the Wav2Vec2 base pretrained encoder substantially degrade the performance of both Wav2Vec2 ﬁne-tuned models, but have very limited effect on any other model. With CW we even observe partial transferability from Wav2Vec2-Large to Wav2Vec2-base (WER = 62.65), while these models have different architectures and were not using the same initial weights. This shows that part of this transferability is indeed due to selfsupervised pretraining, and not just to weight initialization.
---------------------
To our knowledge, this is the ﬁrst case of targeted and transferable adversarial attack on ASR models. These ﬁndings are particularly worrisome given the recent trend in Natural Language Processing, consisting in training a few large selfsupervised models that are applied to many contexts with small amounts of ﬁne-tuning or even zero-shot prompting. Should Speech Recognition take a similar path, ASR adversarial attacks may become a much greater concern.
---------------------
In this work we have shown the relative strength of Transformers vs LSTMs and LAS vs CTC models, with respect to adversarial attacks. We have updated the state of multiple standard attacks against recent ASR models. Finally, we have illustrated the surprising vulnerability of Wav2Vec2 to transferred targeted attacks compared to all other ASR architectures studied in the past. Our results hint towards self-supervised pretraining as a sufﬁcient condition for the transferability of ASR adversarial examples.
---------------------
Direct extensions of this work involve evaluating more attacks, like stronger Black box attacks [10]; and applying attacks to Transducer ASR models, which are popular in streaming Speech Recognition applications [32].. More generally, we hope that our methodology, ﬁndings and code will help accelerate research efforts on ASR robustness.
---------------------
Since no pretrained Transducer is currently provided with Speechbrain, these extensions require important computational efforts.
---------------------
[1] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
---------------------
I. Goodfellow, and R. Fergus, “Intriguing properties of neural networks,” in ICLR, 2014. [Online]. Available: http: //arxiv.org/abs/1312.6199
---------------------
[2] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” 2014.
---------------------
[3] Amazon, “Amazon alexa,” 2021. [Online]. Available: https: //www.amazon.com/b?ie=UTF8&node=21341306011
---------------------
[4] R. Bharadwaj, “Voice and speech recognition in banking – what’s possible to- day,” 2019. [Online]. Available: https: //www.amazon.com/b?ie=UTF8&node=21341306011
---------------------
[5] D. A. et al., “Deep speech 2 : End-to-end speech recognition in english and mandarin,” in Proceedings of The 33rd International Conference on Machine Learning, M. F. Balcan and K. Q. Weinberger, Eds., vol. 48. New York, New York, USA: PMLR, 20–22 Jun 2016, pp. 173–182.
---------------------
[6] P. Warden, “Speech commands: A dataset for limited-vocabulary speech recognition,” CoRR, vol. abs/1804.03209, 2018. [Online]. Available: http://arxiv.org/abs/1804.03209
---------------------
[7] M. M. Cisse, Y. Adi, N. Neverova, and J. Keshet, “Houdini: Fooling deep structured visual and speech recognition models with adversarial examples,” in Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/2017/ ﬁle/d494020ff8ec181ef98ed97ac3f25453-Paper.pdf
---------------------
[8] N. Carlini and D. A. Wagner, “Audio adversarial examples: Targeted attacks on speech-to-text,” CoRR, 2018.
---------------------
[9] M. Alzantot, B. Balaji, and M. Srivastava, “Did you hear that? adversarial examples against automatic speech recognition,” arXiv preprint arXiv:1801.00554, 2018.
---------------------
[10] R. Taori, A. Kamsetty, B. Chu, and N. Vemuri, “Targeted adversarial examples for black box audio systems,” in 2019 IEEE Security and Privacy Workshops (SPW), 2019, pp. 15–20.
---------------------
[11] Y. Qin, N. Carlini, G. Cottrell, I. Goodfellow, and C. Raffel, “Imperceptible, robust, and targeted adversarial examples for automatic speech recognition,” in Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 09–15 Jun 2019, pp. 5231–5240. [Online]. Available: http://proceedings.mlr.press/ v97/qin19a.html
---------------------
[12] Y. Xie, Z. Li, C. Shi, J. Liu, Y. Chen, and B. Yuan, “Enabling fast and universal audio adversarial attack using generative model,” in AAAI, 2021.
---------------------
[13] N. Carlini and D. A. Wagner, “Towards evaluating the robustness of neural networks,” CoRR, 2016.
---------------------
[14] F. Croce, M. Andriushchenko, V. Sehwag, E. Debenedetti,
---------------------
N. Flammarion, M. Chiang, P. Mittal, and M. Hein, “Robustbench: a standardized adversarial robustness benchmark,” in NeurIPS Datasets and Benchmarks Track, 2021. [Online]. Available: https://openreview.net/forum?id=SSKZPJCt7B
---------------------
[15] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep learning models resistant to adversarial attacks,” in ICLR 2018, Conference Track Proceedings, 2018.
---------------------
[16] J. M. Cohen, E. Rosenfeld, and J. Z. Kolter, “Certiﬁed adversarial robustness via randomized smoothing,” CoRR, 2019.
---------------------
[17] R. Olivier and B. Raj, “Sequential randomized smoothing for adversarially robust speech recognition,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 6372–6386. [Online]. Available: https://aclanthology.org/2021. emnlp-main.514
---------------------
[18] H. Abdullah, K. Warren, V. Bindschaedler, N. Papernot, and
---------------------
P. Traynor, “SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identiﬁcation Systems,” in IEEE Symposium on Security and Privacy (IEEE S&P), 2021. [19] H. Abdullah, A. Karlekar, V. Bindschaedler, and P. Traynor, “Demystifying limited adversarial transferability in automatic speech recognition systems,” in International Conference on Learning Representations, 2022. [Online]. Available: https: //openreview.net/forum?id=l5aSHXi8jG5 [20] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell,
---------------------
L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. Chou, S.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Rastorgueva, F. Grondin, W. Aris, H. Na, Y. Gao, R. D. Mori, and Y. Bengio, “SpeechBrain: A general-purpose speech toolkit,” 2021, arXiv:2106.04624. [21] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples,” in ICML, 2018. [Online]. Available: https://arxiv.org/abs/1802.00420 [22] H. Abdullah, M. S. Rahman, W. Garcia, L. Blue, K. Warren,
---------------------
A. S. Yadav, T. Shrimpton, and P. Traynor, “Hear “no evil”, see “Kenansville”: Efﬁcient and Transferable Black-box Attacks on Speech Recognition and Voice Identiﬁcation Systems,” in IEEE Symposium on Security and Privacy (IEEE S&P), 2021. [23] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in ICLR (Poster), 2015. [Online]. Available: http://arxiv.org/abs/1412.6980 [24] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4960–4964. [25] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec:
---------------------
Unsupervised Pre-Training for Speech Recognition,” in Proc. Interspeech 2019, 2019, pp. 3465–3469. [26] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 12 449– 12 460. [Online]. Available: https://proceedings.neurips.cc/paper/ 2020/ﬁle/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf [27] M. Kim, J. Tack, and S. J. Hwang, “Adversarial self-supervised contrastive learning,” in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 2983–2994. [Online]. Available: https://proceedings.neurips.cc/ paper/2020/ﬁle/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf [28] H. Wu, B. Zheng, X. Li, X. Wu, H. Lee, and H. Meng, “Characterizing the adversarial vulnerability of speech selfsupervised learning,” CoRR, vol. abs/2111.04330, 2021. [Online]. Available: https://arxiv.org/abs/2111.04330 [29] N. Papernot, P. D. McDaniel, and I. J. Goodfellow, “Transferability in machine learning: from phenomena to black-box attacks using adversarial samples,” CoRR, 2016. [30] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An asr corpus based on public domain audio books,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 5206–5210. [31] T. W. et al., “Transformers: State-of-the-art natural language processing,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics, Oct. 2020, pp. 38–45. [Online]. Available: https: //www.aclweb.org/anthology/2020.emnlp-demos.6 [32] Y. e. a. He, “Streaming end-to-end speech recognition for mobile devices,” in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 6381–6385.
---------------------
